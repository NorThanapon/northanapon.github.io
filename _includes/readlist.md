## Reading List

##### Language Modeling
* [Intelligible Language Modeling with Input Switched Affine Networks](https://arxiv.org/abs/1611.09434)  
    Jakob N. Foerster, Justin Gilmer, Jan Chorowski, Jascha Sohl-Dickstein, David Sussillo
* [Language Modeling with Gated Convolutional Networks](https://arxiv.org/abs/1612.08083)  
    Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier

##### RL, Memory, and Attention
* [Memory Augmented Neural Networks with Wormhole Connections](https://arxiv.org/abs/1701.08718)  
    Caglar Gulcehre, Sarath Chandar, Yoshua Bengio

##### Generative Adversarial, Highway, and Residual Network
* [Maximum-Likelihood Augmented Discrete Generative Adversarial Networks](https://arxiv.org/abs/1702.07983)  
    Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, Yoshua Bengio
* [Training Very Deep Networks](https://arxiv.org/abs/1507.06228)  
    Rupesh Kumar Srivastava, Klaus Greff, JÃ¼rgen Schmidhuber

##### Others
* [Real Multi-Sense or Pseudo Multi-Sense: An Approach to Improve Word Representation](https://arxiv.org/abs/1701.01574)  
    Haoyue Shi, Caihua Li, Junfeng Hu
* [Understanding Neural Networks through Representation Erasure](https://arxiv.org/abs/1612.08220)  
    Jiwei Li, Will Monroe, Dan Jurafsky
* [Decoder Integration and Expected BLEU Training for Recurrent Neural Network Language Models](https://www.semanticscholar.org/paper/Decoder-Integration-and-Expected-BLEU-Training-for-Auli-Gao/86151fd48b2578ac1232bd927e07a8815144496a)  
    Michael Auli, Jianfeng Gao
